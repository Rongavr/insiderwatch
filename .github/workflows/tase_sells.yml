name: TASE Sell Alerts (2x daily)
run-name: "TASE — ${{ github.event_name }}"

on:
  workflow_dispatch:
    inputs:
      tase_links:
        description: "Space/newline-separated Maya URLs to parse (leave blank for auto-scan)"
        required: false
        default: ""
  schedule:
    # UTC times ≈ IL 09:45 / 17:35 → 07:45 / 15:35 UTC
    - cron: "45 7 * * *"
    - cron: "35 15 * * *"

permissions:
  contents: write  # needed for the auto-commit step to push .tase_state.txt

concurrency:
  group: tase-sells
  cancel-in-progress: true

jobs:
  tase:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    env:
      FROM_EMAIL:    ${{ secrets.FROM_EMAIL }}
      MAIL_USERNAME: ${{ secrets.MAIL_USERNAME }}
      MAIL_PASSWORD: ${{ secrets.MAIL_PASSWORD }}
      SMTP_SERVER:   ${{ secrets.SMTP_SERVER }}
      SMTP_PORT:     ${{ secrets.SMTP_PORT }}
      TO_EMAIL:      ${{ secrets.TO_EMAIL }}

      # If you manually "Run workflow", whatever you type in the form is used here.
      # On scheduled runs this is empty → auto-scan mode.
      TASE_LINKS:    ${{ inputs.tase_links }}

      # Auto-scan controls (used when TASE_LINKS is empty)
      TASE_LAST_ID:    "1702700"   # one-time seed; script persists progress in .tase_state.txt
      TASE_SCAN_AHEAD: "180"       # cap number of IDs probed per run (~a few minutes)
      TASE_SLEEP:      "0.25"      # polite delay between fetches (seconds)
      TIME_BUDGET_S:   "420"       # target ~7 min runtime (guarded below with `timeout`)
      MIN_NIS:         "0"         # filter threshold (0 = show all)

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml pandas python-dateutil pdfminer.six

      - name: Run TASE scanner (guarded & tolerate empty)
        shell: bash
        run: |
          set +e
          # Hard-stop the Python after TIME_BUDGET_S seconds to avoid job timeout
          timeout "${TIME_BUDGET_S}s" python scripts/tase_scan.py
          code=$?
          set -e

          # If the script "fails", treat known benign cases as success
          if [ $code -ne 0 ]; then
            if [ -f tase.log ] && grep -q "No rows this run" tase.log; then
              echo "No rows; marking success."
            elif [ $code -eq 124 ]; then
              echo "Scanner hit shell timeout (${TIME_BUDGET_S}s) — proceeding with partial results."
            else
              echo "Scanner failed with exit $code"
              exit $code
            fi
          fi

          # Ensure CSVs exist (headers only) so artifact upload always works
          python - <<'PY'
import os, csv
specs = [
  ("tase_trades.csv", ["company","tase_code","insider","relation","action","amount_shares","price_nis","est_total_nis","when"]),
  ("tase_alerts.csv", ["company","tase_code","trades","est_total_nis","when"]),
]
for path, headers in specs:
    if not os.path.exists(path) or os.path.getsize(path) == 0:
        with open(path, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow(headers)
PY

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: tase_outputs
          path: |
            tase_trades.csv
            tase_alerts.csv
            tase.log
            .tase_state.txt
          if-no-files-found: ignore

      - name: Persist scanner state
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(tase): update scanner state"
          file_pattern: .tase_state.txt
